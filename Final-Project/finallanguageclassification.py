# -*- coding: utf-8 -*-
"""FinalLanguageClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/162MlavzWuuloq6ZiS3Ow2dJJ4FQefU9X
"""

import pandas as pd
import numpy as np
import string
import resource
import os, re
import array

from google.colab import drive

drive.mount('/content/gdrive')
dataset_dw = pd.read_csv("/content/gdrive/MyDrive/dataset_large_dw.csv")
dataset_wikipedia_test = pd.read_csv("/content/gdrive/MyDrive/dataset_wikipedia.csv")

# big_dataset = pd.concat([dataset_wikipedia, dataset_dw], ignore_index=True)
big_dataset = dataset_dw

def preprocess(big_dataset):  
  big_dataset["ID"] = big_dataset["ID"].map(lambda name: name.lower())
  special = re.compile(r'\W')
  single = re.compile(r'\s+', flags=re.I)
  number = re.compile(r"[-+]?[0-9]+")
  pnumber = re.compile(r"[-+]?[\u06F0-\u06F90-9]+")
  anumber = re.compile(r"[-+]?[\u0660-\u0669]+")
  url = re.compile(r"https?://\S+|www\.\S+")
  html = re.compile(r"<.*?>")
  emoji_pattern = re.compile(
      "["
      u"\U0001F600-\U0001F64F"  # emoticons
      u"\U0001F300-\U0001F5FF"  # symbols & pictographs
      u"\U0001F680-\U0001F6FF"  # transport & map symbols
      u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
      u"\U00002702-\U000027B0"
      u"\U000024C2-\U0001F251"
      "]+",
      flags=re.UNICODE,
  )

  big_dataset["ID"] = big_dataset["ID"].map(lambda x: url.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: html.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: emoji_pattern.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: number.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: pnumber.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: anumber.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: x.translate(str.maketrans(" ", " ", string.punctuation)))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: special.sub(r" ",x))
  big_dataset["ID"] = big_dataset["ID"].map(lambda x: single.sub(r" ", x))

  return big_dataset

big_dataset = preprocess(big_dataset)
dataset_wikipedia_test = preprocess(dataset_wikipedia_test)
df = big_dataset.copy()
df.Category = df.Category.astype('category')
nb_classes = df.Category.cat.codes.max() + 1
df["CategoryCodes"] = df.Category.cat.codes

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(df.ID, df.Category, test_size=0.1, random_state=42)

import math

def vec_fit_transform(X_train):
  DF = {}
  for index in X_train.index:
      for w in X_train[index].split():
          try:
              DF[w].add(index)
          except:
              DF[w] = {index}

  N = len(X_train)

  IDF = DF
  for i in DF:
    # IDF[i] = math.log((N + 1) / float((len(IDF[i]) + 1))) + 1
    IDF[i] = math.log((N) / float((len(IDF[i]) + 1)))


  TF_IDF = {}
  VECTORIZE = dict()
  for index in X_train.index:
      tokens = X_train[index].split()
      words_count = len(tokens)
      weight_list = []
      for token in tokens:
          TF = list(tokens).count(token) / float(words_count)
          RES = TF*IDF[token]
          TF_IDF[index, token] = RES
          weight_list.append(RES)
      try:
          VECTORIZE[index].append(weight_list)
      except:
          VECTORIZE[index] = weight_list

  index_mapper = {}
  index_mapper_cnt = 0
  for i in list(X_train.index):
    index_mapper[i] = index_mapper_cnt
    index_mapper_cnt += 1

  print(index_mapper)

  VOCAB_LIST = [x for x in DF]
  D = np.zeros((len(X_train), len(VOCAB_LIST)))
  for i in TF_IDF:
      # print(i)
      ind = VOCAB_LIST.index(i[1])
      D[index_mapper[i[0]]][ind] = TF_IDF[i]
  print(D[0])

  DATAFRAME = pd.DataFrame(D)

  print(DATAFRAME)
  # DATAFRAME.apply(np.linalg.norm, axis=1)
  # print(DATAFRAME)

  return DATAFRAME, IDF, DF, len(DF)

import math

def vec_transform(X_test, IDF, DF):

  TF_IDF = {}
  VECTORIZE = dict()
  for index in X_test.index:
      tokens = X_test[index].split()
      words_count = len(tokens)
      weight_list = []
      for token in tokens:
          TF = list(tokens).count(token) / float(words_count)
          try:
            RES = TF*IDF[token]
          except:
            RES = 0
          TF_IDF[index, token] = RES
          weight_list.append(RES)
      try:
          VECTORIZE[index].append(weight_list)
      except:
          VECTORIZE[index] = weight_list

  index_mapper = {}
  index_mapper_cnt = 0
  for i in list(X_test.index):
    index_mapper[i] = index_mapper_cnt
    index_mapper_cnt += 1

  print(index_mapper)

  VOCAB_LIST = [x for x in DF]
  D = np.zeros((len(X_test), len(VOCAB_LIST)))
  for i in TF_IDF:
      # print(i)
      try:
        ind = VOCAB_LIST.index(i[1])
        D[index_mapper[i[0]]][ind] = TF_IDF[i]
      except:
        pass
  print(D[0])

  DATAFRAME = pd.DataFrame(D)

  print(DATAFRAME)
  # DATAFRAME.apply(np.linalg.norm, axis=1)
  # print(DATAFRAME)

  return DATAFRAME

vectorized_X_train, IDF, DF, vocab_size = vec_fit_transform(X_train)
# vocab_size = 6

vectorized_X_test = vec_transform(X_test, IDF, DF)

dataset_wikipedia_test.ID

wiki_vectorized_X_test = vec_transform(dataset_wikipedia_test.ID, IDF, DF)

dataset_wikipedia_test.Category

# from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV



# classifier = RandomForestClassifier(n_estimators=200, random_state=0)
# classifier = SGDClassifier(loss="modified_huber", max_iter=10000, alpha=0.00001)
classifier = LinearSVC()
# classifier = LogisticRegression(max_iter=1000)


classifier.fit(vectorized_X_train, y_train)

############################################################################# splited test set

y_pred = classifier.predict(vectorized_X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print(accuracy_score(y_test, y_pred))

############################################################################# wikipedia test set

y_pred = classifier.predict(wiki_vectorized_X_test)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(dataset_wikipedia_test.Category,y_pred))
print(classification_report(dataset_wikipedia_test.Category,y_pred))
print(accuracy_score(dataset_wikipedia_test.Category, y_pred))

############################################################################# kaggle test set

df_test = pd.read_csv('/content/gdrive/MyDrive/kaggle.csv')
print(df_test)
df_test_values = df_test["Id"].copy()
df_test.columns = ['ID', 'Category']
df_test = preprocess(df_test)
df_test.columns = ['Id', 'Category']

print(df_test)
X = vec_transform(df_test["Id"], IDF, DF)
Y_final = classifier.predict(X)

df_test_res = df_test.copy()
df_test_res["Category"] = Y_final
df_test_res["Id"] = df_test_values
print(df_test_res)

# df_test_res = df_test_res.rename(columns = {"Id":"Category"}) 
df_test_res.to_csv("Final.csv", index=False)

def sgd_optimize(X_train, y_train, vocab_size, lr_rate, lr_rate_variation, n_epochs, power_epoch):

    lang_mapper = {"English":0, "German":1, "French":2, "Persian":3, "Arabic":4, "Pashto":5}

    try:
        with open('weights.npy', 'rb') as f:
          w_coeff = np.load(f)
          b_coeff = np.load(f)
          print("Weights File is available!")
    except:
        w_coeff=np.zeros((vocab_size,1)) #Randomly initalizing weights
        b_coeff=np.zeros((1,1))  #Randomly picking up intercept value.
        print("Weights File is not available!")

    
    for epoch in range(1,n_epochs+1):
        sum_errors = 0 #Sum of squared loss.
        N = X_train.shape[0] #The variable N in the SGD equation.


        for i in range(N):
            batch_size = np.random.randint(0,N)  # random batch size for every iteration i.e k batch_size
            


            X_i = X_train[batch_size,:].reshape(1,X_train.shape[1])
            # print(np.array([batch_size]))

            y_i = np.array(lang_mapper[y_train[batch_size]]).reshape(1,1)
            # y_i = y_train[batch_size].reshape(1,1)


            y_pred = np.dot(X_i,w_coeff) + b_coeff                  #y_curr = WT.X + B
            # print(y_pred)
            # print(y_i)
            loss = y_pred - y_i                                     #w_grad = (-2/N)*(X)*(y-(WT.X+B)) = (-2/N)*(X) * loss 
            sum_errors += loss**2                                   #b_grad = (-2/N)*(y-(WT.X+B)) = (-2/N) * loss

            w_grad = X_i.T.dot((y_pred - y_i))
            b_grad = (y_pred - y_i)

            w_coeff = w_coeff -(2/N)*lr_rate*(w_grad)
            b_coeff = b_coeff - (2/N)*lr_rate*(b_grad)

        print("Epoch: %d, Loss: %.3f" %(epoch, sum_errors/N))

        if(lr_rate_variation=='invscaling'): #Implementing learning_rate 'invscaling' similar to that present in SGD Regressor.
            lr_rate = lr_rate / pow(epoch, power_epoch)
        else:
            pass

    return w_coeff, b_coeff

#This function is used to predict the class values given a test data.
def predict(X_test, w_coeff, b_coeff):
    lang_mapper = {0:"English", 1:"German", 2:"French", 3:"Persian", 4:"Arabic", 5:"Pashto"}
    X_test=np.array(X_test)
    y_pred =[]
    for i in range(0,len(X_test)):
        # print(w_coeff)
        # print(X_test[i].reshape(1, -1))
        # print(b_coeff)
        y=np.asscalar(np.dot(w_coeff,X_test[i]) + b_coeff) #Convert an array of size 1 to its scalar equivalent.
        y_pred.append(y)
        # y_pred.append(lang_mapper[int(round(y))])
    return np.array(y_pred)



w_coeff_optimal, b_coeff_optimal = sgd_optimize(vectorized_X_train.to_numpy(), y_train.to_numpy(), vocab_size,lr_rate=1, lr_rate_variation='constant', n_epochs=250, power_epoch=None)

with open('weights.npy', 'wb') as f:
    np.save(f, w_coeff_optimal)
    np.save(f, b_coeff_optimal)
from google.colab import files
files.download('weights.npy')

############################################################################# splited test set

y_pred_manual = predict(vectorized_X_test.to_numpy(), w_coeff_optimal.T, b_coeff_optimal)
y_pred_manual = np.interp(y_pred_manual, (y_pred_manual.min(), y_pred_manual.max()), (0,5))
lang_mapper = {0:"English", 1:"German", 2:"French", 3:"Persian", 4:"Arabic", 5:"Pashto"}
squarer = lambda t: lang_mapper[int(round(t))]
vfunc = np.vectorize(squarer)
y_pred_manual = vfunc(y_pred_manual)


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(y_test,y_pred_manual))
print(classification_report(y_test,y_pred_manual))
print(accuracy_score(y_test, y_pred_manual))

############################################################################# wikipedia test set

y_pred_manual = predict(wiki_vectorized_X_test.to_numpy(), w_coeff_optimal.T, b_coeff_optimal)
y_pred_manual = np.interp(y_pred_manual, (y_pred_manual.min(), y_pred_manual.max()), (0,5))
lang_mapper = {0:"English", 1:"German", 2:"French", 3:"Persian", 4:"Arabic", 5:"Pashto"}
squarer = lambda t: lang_mapper[int(round(t))]
vfunc = np.vectorize(squarer)
y_pred_manual = vfunc(y_pred_manual)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
print(confusion_matrix(dataset_wikipedia_test.Category,y_pred_manual))
print(classification_report(dataset_wikipedia_test.Category,y_pred_manual))
print(accuracy_score(dataset_wikipedia_test.Category, y_pred_manual))